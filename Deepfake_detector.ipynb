{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgDDx+4RZD/8/eGZnO4TT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mark-polo/machine-learning/blob/main/Deepfake_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils.py"
      ],
      "metadata": {
        "id": "xGwkSbKwJORO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The code is a Python module that contains several functions for loading, manipulating and preparing data for a\n",
        "DeepFake video detection project.\n",
        "\n",
        "It imports the following libraries:\n",
        "        - os\n",
        "        - cv2\n",
        "        - numpy\n",
        "        - pandas\n",
        "        - tensorflow.keras.\n",
        "\n",
        "Methods and them describe:\n",
        "_____\n",
        "\n",
        "    create_df(json_path: str = None, file_paths: np.ndarray = None, file_labels: np.ndarray = None, mode: int = 0): This\n",
        "    function creates a Pandas DataFrame either from a JSON file or from passed data. It takes four arguments: json_path (\n",
        "    a string that is a path to a JSON file), file_paths (a Numpy array of the paths which was get from filename),\n",
        "    file_labels (a Numpy array of the labels which was get from filename), and mode (an integer that takes values 0 and\n",
        "    1. Value 0 means that data to creating a dataframe take from the JSON file, and 1 means data be transferred to the\n",
        "    method). The function returns the created dataframe, labels, path list and a dataframe created from passed data.\n",
        "\n",
        "    play_video(path: str, title: str = \"\"): This function displays a video by using the OpenCV library. It takes two\n",
        "    arguments: path (a string that is a path to the video file) and title (a string that is the title of the video). The\n",
        "    function returns nothing.\n",
        "\n",
        "    crop_center_square(frame): This function is used to reduce the size of frames and create new images with size (\n",
        "    224x224). If we just try to resize images by size (224x224), we will get poor quality squeezing images. It takes one\n",
        "    argument frame (a frame from the video and preparing them) and returns a new image.\n",
        "\n",
        "    load_video(path: str, max_frames: int = 0, resize=(img_size, img_size)) -> np.ndarray: This function loads a video,\n",
        "    resizes the frames and changes color space from BGR to RGB. All images are saved to an array and then put to the\n",
        "    numpy array because the machine learning model works with numpy array. It takes three arguments: path (a string that\n",
        "    is a path to the video file), max_frames (an integer that checks the emptiness list of frames and if it has then\n",
        "    break), and resize (a tuple of the image size, default 224x224). The function returns a numpy array of the frames.\n",
        "\n",
        "    pretrain_feature_extractor(): This function is used to extract meaningful features from images by means of the\n",
        "    pretrained Inception V3 model with Imagenet weights. The function returns a pretrained model.\n",
        "\n",
        "Example:\n",
        "------\n",
        "    For creating dataframe:\n",
        "        df_json, df, labels, label_list, path_list = create_df(\n",
        "            \"path\")\n",
        "    If we're using prepare video finction:\n",
        "        train_data, train_labels = prepare_all_videos(train, train_path, feature_extractor)\n",
        "    And when we're using predict:\n",
        "        prediction(path=test_videos, feature_extractor=feature_extractor, model=model)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def create_df(json_path: str = None, file_paths: np.ndarray = None, file_labels: np.ndarray = None, mode: int = 0):\n",
        "    \"\"\"\n",
        "    Simply way to create a dataframe from json file using pandas function read_json. And also from passed data into\n",
        "    method.\n",
        "\n",
        "    :param mode: Takes a values 0 and 1. Value 0 means that data to creating dataframe take from json file ,\n",
        "    and 1 means data be transferred to method\n",
        "    :param file_labels: Takes a numpy array of the labels which was get from filename\n",
        "    :param file_paths:Takes a numpy array of the paths which was get from filename\n",
        "    :param json_path: Takes a path to the json file\n",
        "    :return: A dataframe created from json file, dataframe created from passed data, labels and filenames\n",
        "    \"\"\"\n",
        "\n",
        "    # create df from json file\n",
        "    df_json = pd.read_json(json_path).T\n",
        "    # go through list of the indexes (df_json.index) and with using global variable TRAIN_PATH,\n",
        "    # create path to the images\n",
        "    path_list = list(map(lambda i: os.path.join(train_path, i), df_json.index))\n",
        "    # go through labels of the df and save them in list\n",
        "    label_list = list(df_json[\"label\"])\n",
        "\n",
        "    if mode == 0:\n",
        "        # create df\n",
        "        filepath = pd.Series(data=path_list, name=\"Path\").astype(str)\n",
        "        labels = pd.Series(data=label_list, name=\"Label\")\n",
        "        df = pd.concat([filepath, labels], axis=1)\n",
        "    elif mode == 1:\n",
        "        # create df\n",
        "        filepath = pd.Series(data=file_paths, name=\"Path\").astype(str)\n",
        "        labels = pd.Series(data=file_labels, name=\"Label\")\n",
        "        df = pd.concat([filepath, labels], axis=1)\n",
        "    else:\n",
        "        # If mode get wrong value , will get error\n",
        "        raise Exception(\"Sorry, you are pass wrong value , parameter mode takes only 0 and 1\")\n",
        "    return df_json, df, labels, label_list, path_list\n",
        "\n",
        "\n",
        "def play_video(path: str, title: str = \"\"):\n",
        "    \"\"\"\n",
        "    This is simply method to showing video by using opencv library.\n",
        "\n",
        "    :param path: Takes a path to the video.\n",
        "    :param title: Takes a title\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    print(path)\n",
        "    video = cv.VideoCapture(path)\n",
        "    while video.isOpened():\n",
        "        _, frame = video.read()\n",
        "        cv.imshow(title, frame)\n",
        "        # Press esc to escape\n",
        "        key = cv.waitKey(1)\n",
        "        if key == 27:\n",
        "            break\n",
        "    # When everything done, release\n",
        "    # the video capture object\n",
        "    video.release()\n",
        "    # closing all open windows\n",
        "    cv.destroyAllWindows()\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    \"\"\"\n",
        "    This is method using to reduce size frames and creating new images with size (224x224).If just try to\n",
        "    resize images by size (224x224) , you're got poor quality squeezing images\n",
        "\n",
        "    :param frame: Takes a frames from video and preparing them\n",
        "    :return: A new images\n",
        "    \"\"\"\n",
        "\n",
        "    h, w = frame.shape[0:2]\n",
        "    min_dim = min(h, w)\n",
        "    center_x = (w // 2) - (min_dim // 2))\n",
        "    center_y = (h // 2) - (min_dim // 2)\n",
        "    return frame[center_y: center_y + min_dim, center_x: center_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path: str, _frames: int = 0, resize=(img_size, img_size)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This is method to loading the video. Here video resizing  and changing color space from BGR\n",
        "    to RGB. All images saving to the array and then put to the numpy array , because the machine learning\n",
        "    model working with numpy array.\n",
        "\n",
        "    :param path: Takes a path to the video.\n",
        "    :param _frames: Takes a value that checks emptiness list of frames and if is has then break\n",
        "    :param resize: Takes a tuple of the images size default 224x224\n",
        "    :return: A numpy array of the frames\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while 1:\n",
        "            success, frame = cap.read()\n",
        "            # if success will be false then break , that's mean video can't read\n",
        "            if not success:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv.resize(frame, resize)\n",
        "            # Set RGB color also can use this [2, 1, 0] \"frame[:, :, [2, 1, 0]]\"\n",
        "            frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "            # if you haven't any frames\n",
        "            if len(frames) == _frames:\n",
        "                break\n",
        "    finally:\n",
        "        # When everything done, release\n",
        "        # the video capture object\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def pretrain_feature_extractor():\n",
        "    \"\"\"\n",
        "    This is method using to extract meaningful features from images by means of pretrained Inception V3 model\n",
        "    with imagenet weights\n",
        "    :return: A pretrained model\n",
        "    \"\"\"\n",
        "\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"path\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(img_size, img_size, 3)\n",
        "    )\n",
        "    # preprocessing data between -1 and 1\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((img_size, img_size, 3))\n",
        "    # Here returns tf.Tensor(type float32) instead numpy.array\n",
        "    # The images are converted from RGB to BGR, then each color channel\n",
        "    # is zero-centered with respect to the ImageNet dataset, without scaling.\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "def prepare_all_videos(df: pd.DataFrame, path: str, feature_extractor):\n",
        "    \"\"\"\n",
        "    This method is need for take of the video frames and extract features from them.\n",
        "\n",
        "    'feature_extractor.predict(batch[None, j, :])': the method that takes the most important features of frames of the\n",
        "    video by using the CNN model and reducing dimensions from 224x224x3 (150528) to for train set (2048,) where 2048\n",
        "    count of the features. And whole shape for the RNN model has (280, 30, 2048) where 280 counts of the samples for\n",
        "    train set, 30 counts of the frames and 2048 count of the extracted features, in a case with test set count of the\n",
        "    samples have 120.\n",
        "\n",
        "    'temp_frame_mask[i, :length]': fills 1 (True) means doesn't need to do padding, if the lengths of the frames (not\n",
        "    only frames that techies can apply to text) are different, need to add some paddings and when paddings adding\n",
        "    they are filling False. When padding adds needn't filling by True. Shape here (30,).\n",
        "\n",
        "    Then temporary variables putting to the numpy ndarray with shapes, for features (280/120 , 30, 2048) and mask\n",
        "    (280/120, 30)\n",
        "\n",
        "    :param df: Takes a dataframe\n",
        "    :param path: Takes a path to dataset\n",
        "    :param feature_extractor: Takes a pretrained model which will extract features from frames of the video\n",
        "    :return: A features , masks  and labels to further feeding them to RNN model\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples = len(df)\n",
        "    video_names = list(df.index)\n",
        "    # get labels and preprocessing them\n",
        "    labels = df[\"label\"].values\n",
        "    labels = np.array(labels == \"FAKE\").astype(int)\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what will feed to sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    #  masked with padding or not.\n",
        "    #  MAX_SEQ_LENGTH - timestep (size of sequences)\n",
        "    #  NUM_FEATURES - features (count of the neurons of RNN)\n",
        "    frame_mask = np.zeros(shape=(num_samples, max_seq_length), dtype=\"bool\")\n",
        "    frame_feature = np.zeros(shape=(num_samples, max_seq_length, num_features), dtype=\"float32\")\n",
        "\n",
        "    # go through all images\n",
        "    for idx, items in enumerate(video_names):\n",
        "        frames = load_video(os.path.join(path, items))\n",
        "        frames = frames[None, ...]  # (1, 300, 224, 224, 3)\n",
        "        print(\"Frames shape : \", frames.shape)\n",
        "\n",
        "        # initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
        "\n",
        "        # extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            print(\"Look at the video_length : \", video_length)\n",
        "            # video_length equal to 300 and max_seq_length 30 , so here is always ganna be 30\n",
        "            length = min(max_seq_length, video_length)\n",
        "            print(length)\n",
        "            for j in range(length):\n",
        "                # Go through 30 frames in each images of the all (batch[None, j, :])\n",
        "                # and put results of the prediction\n",
        "                # to the temp_frame_features variable here [i, j, :],\n",
        "                # j - 30 frames of the images\n",
        "                # Shape (2048,)\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "                print(i, \" \", j)\n",
        "                print(temp_frame_features[i, j, :].shape)\n",
        "            # When pass 1 to mask this means nothing will pad.\n",
        "            # Nothing need to add to frame that adjust it to right length\n",
        "            # Shape (30,)\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        # features and masks whose stored in temp variables put to main variables\n",
        "        frame_feature[idx,] = temp_frame_features.squeeze()\n",
        "        frame_mask[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_feature, frame_mask), labels\n",
        "\n",
        "\n",
        "def prepare_single_video(frames, feature_extractor):\n",
        "    \"\"\"\n",
        "    This method is similar to prepare_all_videos , but here don't prepare labels and saved features, mask immediately\n",
        "    to blank.\n",
        "\n",
        "    'feature_extractor.predict(batch[None, j, :])': the method that takes the most important features of frames of the\n",
        "    video by using the CNN model and reducing dimensions from 224x224x3 (150528) to for train set (2048,) where 2048\n",
        "    count of the features. And whole shape for the RNN model has (1, 30, 2048) where 1 count of the samples for\n",
        "    one video, 30 counts of the frames and 2048 count of the extracted features.\n",
        "\n",
        "    'temp_frame_mask[i, :length]': fills 1 (True) means doesn't need to do padding, if lengths of the frames (not only\n",
        "    frames that techies can apply to text) are different, need to add some paddings and when paddings adding they are\n",
        "    filling False. When padding adds needn't filling by True. Shape here (30,).\n",
        "\n",
        "    :param frames: Takes a frames of the video\n",
        "    :param feature_extractor: Takes a pretrained model\n",
        "    :return: A features and mask\n",
        "    \"\"\"\n",
        "\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, num_features,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, num_features, num_features), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(num_features, video_length)\n",
        "        for j in range(length):\n",
        "            # Shape(2048,)\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        # Shape (30,)\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def prediction(feature_extractor, model, path = None, video = None):\n",
        "    \"\"\"\n",
        "    This method is making predict of the model\n",
        "\n",
        "    :param video: Takes a live video\n",
        "    :param path: Takes a path to test directory\n",
        "    :param feature_extractor: Takes a pretrained feature extractor model\n",
        "                              because method prepare_single_video is implementing here\n",
        "    :param model: Takes a already trained model\n",
        "    :return: A prediction\n",
        "    \"\"\"\n",
        "\n",
        "    frames = load_video(path)\n",
        "    features, masks = prepare_single_video(frames if video is None else video, feature_extractor)\n",
        "    return model.predict([features, masks])[0]"
      ],
      "metadata": {
        "id": "C_HQ1oM2JQ8x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rnn.py"
      ],
      "metadata": {
        "id": "oM5hymn_JXIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module contains a class to create an RNN model for training.\n",
        "\n",
        "This module contains the following class:\n",
        "RNN: A class to create an RNN model for training.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    \"\"\"\n",
        "    This class is used to create a RNN model.\n",
        "\n",
        "    Attributes:\n",
        "        rate (float): A dropout rate.\n",
        "        units (int): A count of the outputs in last dense layer in the RNN model.\n",
        "\n",
        "    Methods:\n",
        "        training():\n",
        "        This method creates and returns the RNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rate, units):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the RNN class.\n",
        "\n",
        "        Args:\n",
        "            rate (float): A dropout rate.\n",
        "            units (int): A count of the outputs in last dense layer in the RNN model.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "\n",
        "        \"\"\"\n",
        "        self.rate = rate\n",
        "        self.units = units\n",
        "\n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        This method creates and returns the RNN model.\n",
        "\n",
        "        Args:\n",
        "            None.\n",
        "\n",
        "        Returns:\n",
        "            model (tensorflow.python.keras.engine.functional.Functional): A RNN model.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Define inputs\n",
        "        features_input = keras.Input((max_seq_length, num_features))\n",
        "        mask_input = keras.Input((max_seq_length,), dtype=\"bool\")\n",
        "\n",
        "        # Define layers\n",
        "        # return_sequences=True - (Many to Many) for each recursion get a values and make\n",
        "        # a tensor with shape (batch, timestep, features) and can be linking with next layer\n",
        "        x = keras.layers.GRU(32, return_sequences=True)(features_input, mask=mask_input)\n",
        "        x = keras.layers.GRU(16)(x)\n",
        "        x = keras.layers.Dropout(self.rate)(x)\n",
        "        x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        output = keras.layers.Dense(self.units, activation=\"sigmoid\")(x)\n",
        "\n",
        "        # Define model\n",
        "        model = keras.Model([features_input, mask_input], output)\n",
        "        # Compile model\n",
        "\n",
        "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\n",
        "\n",
        "        # Display model summary\n",
        "        model.summary()\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "D5h5ctmAJZKl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global_variabels.py"
      ],
      "metadata": {
        "id": "lULd_wQzJfux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"path\"\n",
        "img_size = 224\n",
        "batch_size = 8\n",
        "epochs = 30\n",
        "max_seq_length = 50  # timestep (size of sequences)\n",
        "num_features = 2048  # feature (count of the neurons of RNN)\n",
        "units = 1  # count of the outputs in last dense layer in the rnn model one because model will be\n",
        "# trained to distinguish a FAKE videos from a REAL.\n",
        "test_path = \"path\"\n",
        "checkpoint_path = \"path\""
      ],
      "metadata": {
        "id": "rpEbOWiBJlA-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coach.py"
      ],
      "metadata": {
        "id": "a91BMIzxJ5Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module contains the fit() function for training a deep learning model for video classification.\n",
        "\n",
        "It uses the following modules:\n",
        "    - os\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - keras.models.load_model\n",
        "    - sklearn.model_selection.train_test_split\n",
        "\n",
        "\n",
        "Parameters:\n",
        "-----------\n",
        "    model_name : str, optional\n",
        "        A string value used to name the model.\n",
        "    pre_trained_model_path : str, optional\n",
        "        Path to the pre-trained model.\n",
        "    load_video_file : str, optional\n",
        "        Path to the video file for which the user wants to make a prediction.\n",
        "    mode : int, optional\n",
        "        Integer value indicating the mode. 0 means the model will be trained and saved, 1 means a saved model will be used, and 2 means live video will be used.\n",
        "    to : int, optional\n",
        "        The end index of the images used for validation.\n",
        "    frm : int, optional\n",
        "        The start index of the images used for validation.\n",
        "\n",
        "\n",
        "Returns:\n",
        "--------\n",
        "    None\n",
        "\n",
        "\n",
        "Example:\n",
        "--------\n",
        "For training model from scratch:\n",
        "    fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=0)\n",
        "\n",
        "For situation where model trained and mode equal 1:\n",
        "    fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=1, frm=10, to=100)\n",
        "\n",
        "In situation where model trained and mode equal 2:\n",
        "    fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=2)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def fit(model_name: str = None, pre_trained_model_path: str = None, load_video_file: str = None, mode: int = 0, frm: int = 0, to: int = 200) -> None:\n",
        "    \"\"\"\n",
        "    This method is assembling all steps before\n",
        "\n",
        "    :param model_name: A string value used to name the model.\n",
        "    :param load_video_file: Path to the video file for which the user wants to make a prediction.\n",
        "    :param pre_trained_model_path: Path to the pre-trained model.\n",
        "    :param mode: Integer value indicating the mode. 0 means the model will be trained and saved, 1 means a saved model will be used, and 2 means live video will be used.\n",
        "    :param to: The end index of the images used for validation.\n",
        "    :param frm: The start index of the images used for validation.\n",
        "\n",
        "    Example:\n",
        "    __________\n",
        "\n",
        "        For training model from scratch:\n",
        "            fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=0)\n",
        "        For situation where model trained and mode equal 1:\n",
        "            fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=1, frm=10, to=100)\n",
        "        In situation where model trained and mode equal 2:\n",
        "            fit(model_name=\"deepfake_cnn_rnn_5.h5\", pre_trained_model_path=\"/checkpoint/deepfake_cnn_rnn_5.h5\", mode=2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Load feature extractor\n",
        "    feature_extractor = pretrain_feature_extractor()\n",
        "\n",
        "    if mode == 0:\n",
        "        # Load data and split into train and test\n",
        "        df_json, df, labels, label_list, path_list = create_df(\n",
        "            \"path\")\n",
        "\n",
        "        train, test = train_test_split(df_json, test_size=0.3, random_state=42, stratify=df_json['label'])\n",
        "        print(f\"Train set shape: {train.shape}, Test set shape: {test.shape}\")\n",
        "\n",
        "        # Prepare data for training and testing\n",
        "        train_data, train_labels = prepare_all_videos(train, train_path, feature_extractor)\n",
        "        test_data, test_labels = prepare_all_videos(test, train_path, feature_extractor)\n",
        "\n",
        "        print(f\"Train frame features shape: {train_data[0].shape}, Train frame masks shape: {train_data[1].shape}\")\n",
        "        print(f\"Test frame features shape: {test_data[0].shape}, Test frame masks shape: {test_data[1].shape}\")\n",
        "\n",
        "        # Initialize and train model\n",
        "        model = RNN(0.4, units).training()\n",
        "        model.fit(\n",
        "            [train_data[0], train_data[1]],\n",
        "            train_labels,\n",
        "            validation_data=([test_data[0], test_data[1]], test_labels),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        os.chdir(checkpoint_path)\n",
        "        model.save(model_name)\n",
        "\n",
        "    elif mode == 1:\n",
        "        # Load pre-trained model\n",
        "        model = load_model(pre_trained_model_path)\n",
        "\n",
        "        # Select video to predict on\n",
        "        test_df = pd.read_json(\"path\").T\n",
        "        test_paths = list(map(lambda x: os.path.join(test_path, x), test_df.index[frm:to]))\n",
        "        test_videos = np.random.choice(test_paths) if load_video_file is None else load_video_file\n",
        "\n",
        "        # Make prediction and display result\n",
        "        p = prediction(path=test_videos, feature_extractor=feature_extractor, model=model)\n",
        "        if p <= 0.5:\n",
        "            play_video(test_videos, f\"Real with probability {100 - p}\")\n",
        "        else:\n",
        "            play_video(test_videos, f\"Fake with probability {p}\")\n",
        "    elif mode == 2:\n",
        "        detector(model_path=pre_trained_model_path)\n",
        "    else:\n",
        "        raise Exception(\"Sorry, you are pass wrong value , parameter mode takes only 0, 1 and 2\")"
      ],
      "metadata": {
        "id": "cAUWyXbkJ08F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Face_detection.py"
      ],
      "metadata": {
        "id": "V4HH63QsKL8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Detects fake faces in a video stream using a pre-trained deep learning model.\n",
        "\n",
        ":param model_path: Path to the pre-trained Keras model.\n",
        ":param size: Scaling factor for the video frames. Default is 2.\n",
        ":return: The function displays the results in real-time.\n",
        ":raises: If the model_path is invalid or the model cannot be loaded.\n",
        "\n",
        "Example:\n",
        "________\n",
        "\n",
        "    detect_fake_video(\"deepfake_cnn_rnn_5.h5\", size=2)\n",
        "\"\"\"\n",
        "\n",
        "import cv2 as cv\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def detector(model_path: str, size: int = 2) -> None:\n",
        "    \"\"\"\n",
        "    Detects fake faces in a video stream using a pre-trained deep learning model.\n",
        "\n",
        "    :param model_path: Path to the pre-trained Keras model.\n",
        "    :param size: Scaling factor for the video frames. Default is 2.\n",
        "    :return: The function displays the results in real-time.\n",
        "    :raises: If the model_path is invalid or the model cannot be loaded.\n",
        "\n",
        "    Example:\n",
        "    ________\n",
        "\n",
        "        detector(\"deepfake_cnn_rnn_5.h5\", size=2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Load the face cascade classifier\n",
        "    cascade = cv.CascadeClassifier(\"path\")\n",
        "\n",
        "    # Initialize the video capture device\n",
        "    cap = cv.VideoCapture(0)\n",
        "\n",
        "    # Load the pre-trained feature extractor\n",
        "    feature_extractor = pretrain_feature_extractor()\n",
        "\n",
        "    while True:\n",
        "        # Capture a frame from the video stream\n",
        "        _, frame = cap.read()\n",
        "\n",
        "        # Flip the frame horizontally\n",
        "        frame = cv.flip(frame, 1, 1)\n",
        "\n",
        "        # Resize the frame\n",
        "        w = int(frame.shape[1] // size)\n",
        "        h = int(frame.shape[0] // size)\n",
        "        dim = (w, h)\n",
        "        res = cv.resize(frame, dim)\n",
        "\n",
        "        # Detect faces in the frame\n",
        "        faces = cascade.detectMultiScale(res)\n",
        "\n",
        "        for face in faces:\n",
        "            # Scale the face coordinates\n",
        "            (x, y, w, h) = [v * size for v in face]\n",
        "            x = int(x)\n",
        "            y = int(y)\n",
        "            w = int(w)\n",
        "            h = int(h)\n",
        "\n",
        "            # Extract the face from the frame\n",
        "            face_img = frame[y:y + h, x:x + w]\n",
        "\n",
        "            # Resize and reshape the face image for the model\n",
        "            resized = cv.resize(face_img, (224, 224))\n",
        "            reshaped = np.reshape(resized, (1, 224, 224, 3))\n",
        "            reshaped = np.vstack([reshaped])\n",
        "\n",
        "            # Use the trained model to predict whether the face image is real or fake\n",
        "            result = prediction(feature_extractor=feature_extractor, video=reshaped, model=model)\n",
        "\n",
        "            # Display the result on the frame\n",
        "            if result <= 0.5:\n",
        "                cv.putText(frame, \"This is Real\", (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "            else:\n",
        "                cv.putText(frame, \"This is Fake\", (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "\n",
        "        # Display the frame\n",
        "        cv.imshow(\"Fake Detection\", frame)\n",
        "\n",
        "        # Check for exit key\n",
        "        key = cv.waitKey(1)\n",
        "        if key == 27:\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cv.destroyAllWindows()"
      ],
      "metadata": {
        "id": "oabveMzPKOAD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py"
      ],
      "metadata": {
        "id": "IroRFOR-Kn4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "\"\"\"\n",
        "1. python3 main.py --model_name=deepfake_cnn_rnn_6.h5 --mode=0 \n",
        "\n",
        "2. python3 main.py --pre_trained_model_path=path --mode=1 --load_video_file=path\n",
        "\n",
        "3. python3 main.py --pre_trained_model_path=path --mode=2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_script(model_name: str = None, pre_trained_model_path: str = None, load_video_file: str = None, mode: int = 0,\n",
        "               frm: int = 0, to: int = 200):\n",
        "    fit(\n",
        "        model_name=model_name,\n",
        "        pre_trained_model_path=pre_trained_model_path,\n",
        "        load_video_file=load_video_file,\n",
        "        mode=mode,\n",
        "        to=to,\n",
        "        frm=frm\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train or use a deepfake detection model\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=None, help=\"A string value used to name the model\")\n",
        "    parser.add_argument(\"--pre_trained_model_path\", type=str, default=None, help=\"Path to the pre-trained model\")\n",
        "    parser.add_argument(\"--load_video_file\", type=str, default=None,\n",
        "                        help=\"Path to the video file for which to make a prediction\")\n",
        "    parser.add_argument(\"--mode\", type=int, default=0, choices=[0, 1, 2],\n",
        "                        help=\"Integer value indicating the mode. 0 means the model will be trained and saved, \"\n",
        "                             \"1 means a saved model will be used, and 2 means live video will be used\")\n",
        "    parser.add_argument(\"--to\", type=int, default=200, help=\"The end index of the images used for validation\")\n",
        "    parser.add_argument(\"--frm\", type=int, default=0, help=\"The start index of the images used for validation\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    run_script(model_name=args.model_name, pre_trained_model_path=args.pre_trained_model_path,\n",
        "               load_video_file=args.load_video_file, mode=args.mode, to=args.to, frm=args.frm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "awMc26fOKnYA",
        "outputId": "4869d43e-d14f-4f51-f430-3d0a8b3eaf52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--model_name MODEL_NAME]\n",
            "                             [--pre_trained_model_path PRE_TRAINED_MODEL_PATH]\n",
            "                             [--load_video_file LOAD_VIDEO_FILE]\n",
            "                             [--mode {0,1,2}] [--to TO] [--frm FRM]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2d2aceb6-f626-4de2-9fb6-52c5a9765862.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}
